{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c37143-203d-4f03-ae39-32dea7596273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this for croppped out image that i did in labelimg\n",
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def crop_faces(images_dir, labels_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        img_path = os.path.join(images_dir, split)\n",
    "        lbl_path = os.path.join(labels_dir, split)\n",
    "        out_path = os.path.join(output_dir, split)\n",
    "        os.makedirs(out_path, exist_ok=True)\n",
    "        \n",
    "        for label_file in os.listdir(lbl_path):\n",
    "            if not label_file.endswith('.xml'):\n",
    "                continue\n",
    "            tree = ET.parse(os.path.join(lbl_path, label_file))\n",
    "            root = tree.getroot()\n",
    "            filename = root.find('filename').text\n",
    "            label_img_path = os.path.join(img_path, filename)\n",
    "            img = cv2.imread(label_img_path)\n",
    "            for obj in root.iter('object'):\n",
    "                name = obj.find('name').text\n",
    "                bbox = obj.find('bndbox')\n",
    "                xmin = int(bbox.find('xmin').text)\n",
    "                ymin = int(bbox.find('ymin').text)\n",
    "                xmax = int(bbox.find('xmax').text)\n",
    "                ymax = int(bbox.find('ymax').text)\n",
    "                face_crop = img[ymin:ymax, xmin:xmax]\n",
    "                \n",
    "                person_dir = os.path.join(out_path, name)\n",
    "                os.makedirs(person_dir, exist_ok=True)\n",
    "                output_file = os.path.join(person_dir, f\"{filename}{xmin}{ymin}.jpg\")\n",
    "                cv2.imwrite(output_file, face_crop)\n",
    "    print(f\"Face crops saved to {output_dir}\")\n",
    "\n",
    "images_dir = r'C:\\object_detection\\py_T0R\\datasets\\face_worker\\images'\n",
    "labels_dir = r'C:\\object_detection\\py_T0R\\datasets\\face_worker\\labels'\n",
    "output_dir = r'C:\\object_detection\\py_T0R\\datasets\\face_worker\\cropped_faces'\n",
    "crop_faces(images_dir, labels_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bffbd6-c425-4919-afaf-249cec80dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff approach using MTCNN\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "dataset_path = r'C:\\object_detection\\py_T0R\\datasets\\face_worker\\cropped_faces\\train'\n",
    "embedding_file = r'C:\\object_detection\\py_T0R\\datasets\\face_worker\\face_embeddings.pkl'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for img_tensor, label in loader:\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()[0]\n",
    "        embeddings.append(embedding)\n",
    "        labels.append(dataset.classes[label[0]])\n",
    "\n",
    "with open(embedding_file, 'wb') as f:\n",
    "    pickle.dump({'embeddings': np.array(embeddings), 'labels': np.array(labels)}, f)\n",
    "\n",
    "print(f\"âœ… Saved {len(embeddings)} face embeddings to:\\n{embedding_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f1668-a7af-4180-a7e7-33e023d737e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#realtime detection main code\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import warnings\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on:\", device)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load known embeddings\n",
    "with open(r'C:\\object_detection\\py_T0R\\datasets\\face_worker\\face_embeddings.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "known_embeddings = data['embeddings']\n",
    "known_labels = data['labels']\n",
    "\n",
    "# Initialize face detector and recognizer\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    boxes, _ = mtcnn.detect(img_rgb)\n",
    "\n",
    "    if boxes is not None:\n",
    "        faces = mtcnn(img_rgb)  # returns a list of face tensors\n",
    "        for i, face in enumerate(faces):\n",
    "            if face is None:\n",
    "                continue\n",
    "            face_input = face.unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedding = resnet(face_input).cpu().numpy()\n",
    "\n",
    "            similarities = cosine_similarity(embedding, known_embeddings)[0]\n",
    "            best_match_idx = np.argmax(similarities)\n",
    "            best_similarity = similarities[best_match_idx]\n",
    "            name = known_labels[best_match_idx] if best_similarity > 0.75 else \"Unknown\"\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            box = boxes[i].astype(int)\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{name} ({best_similarity:.2f})\", (box[0], box[1]-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aria)",
   "language": "python",
   "name": "aria"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
